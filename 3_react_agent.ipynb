{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReAct Agent with Wikipedia, ArXiv & PubMed\n",
    "\n",
    "This notebook builds a research-focused ReAct agent using **3 free LangChain tools** that require no API key:\n",
    "\n",
    "| Tool | Best for |\n",
    "|---|---|\n",
    "| **Wikipedia** | General knowledge, history, concepts |\n",
    "| **ArXiv** | CS, AI, math, physics academic papers |\n",
    "| **PubMed** | Biology, medicine, health research |\n",
    "\n",
    "The agent uses the **ReAct** (Reasoning + Acting) framework: it thinks step-by-step and automatically picks the right tool for each question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "Run this once. If packages are already installed you can skip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in ./.venv/lib/python3.11/site-packages (0.1.20)\n",
      "Requirement already satisfied: langchain-community in ./.venv/lib/python3.11/site-packages (0.0.38)\n",
      "Requirement already satisfied: langchain-groq in ./.venv/lib/python3.11/site-packages (0.1.2)\n",
      "Requirement already satisfied: wikipedia in ./.venv/lib/python3.11/site-packages (1.4.0)\n",
      "Requirement already satisfied: arxiv in ./.venv/lib/python3.11/site-packages (2.4.0)\n",
      "Requirement already satisfied: pymupdf in ./.venv/lib/python3.11/site-packages (1.27.1)\n",
      "Requirement already satisfied: python-dotenv in ./.venv/lib/python3.11/site-packages (1.2.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.11/site-packages (from langchain) (6.0.3)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.venv/lib/python3.11/site-packages (from langchain) (2.0.46)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.venv/lib/python3.11/site-packages (from langchain) (3.13.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./.venv/lib/python3.11/site-packages (from langchain) (0.6.7)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.52 in ./.venv/lib/python3.11/site-packages (from langchain) (0.1.53)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in ./.venv/lib/python3.11/site-packages (from langchain) (0.0.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in ./.venv/lib/python3.11/site-packages (from langchain) (0.1.147)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./.venv/lib/python3.11/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in ./.venv/lib/python3.11/site-packages (from langchain) (2.12.5)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.11/site-packages (from langchain) (2.32.5)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in ./.venv/lib/python3.11/site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.22.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.venv/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.26.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./.venv/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.52->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in ./.venv/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.52->langchain) (23.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.venv/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.11.7)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./.venv/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.12.1)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in ./.venv/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in ./.venv/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in ./.venv/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.6.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.1.0)\n",
      "Requirement already satisfied: groq<1,>=0.4.1 in ./.venv/lib/python3.11/site-packages (from langchain-groq) (0.37.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.11/site-packages (from groq<1,>=0.4.1->langchain-groq) (1.9.0)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.11/site-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.1)\n",
      "Requirement already satisfied: beautifulsoup4 in ./.venv/lib/python3.11/site-packages (from wikipedia) (4.14.3)\n",
      "Requirement already satisfied: feedparser~=6.0.10 in ./.venv/lib/python3.11/site-packages (from arxiv) (6.0.12)\n",
      "Requirement already satisfied: sgmllib3k in ./.venv/lib/python3.11/site-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in ./.venv/lib/python3.11/site-packages (from beautifulsoup4->wikipedia) (2.8.3)\n",
      "Requirement already satisfied: xmltodict in ./.venv/lib/python3.11/site-packages (1.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain-community langchain-groq wikipedia arxiv pymupdf python-dotenv\n",
    "!pip install xmltodict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports\n",
    "Bring in everything we need: the LLM, agent utilities, and the 3 tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "# Tool classes\n",
    "from langchain_community.tools import WikipediaQueryRun, ArxivQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper, ArxivAPIWrapper\n",
    "from langchain_community.tools.pubmed.tool import PubmedQueryRun\n",
    "\n",
    "# Load GROQ_API_KEY from your .env file\n",
    "load_dotenv() \n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LLM Setup\n",
    "We use **Groq + LLaMA 3.3** as our brain.\n",
    "\n",
    "> `temperature=0` is important here, it makes the LLM follow the strict ReAct format reliably (`Thought → Action → Observation → Final Answer`). Higher values cause formatting errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM ready!\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0,       # Keep at 0 for reliable ReAct formatting\n",
    "    max_tokens=256,\n",
    ")\n",
    "\n",
    "print(\"LLM ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tools Setup\n",
    "\n",
    "We configure each tool individually before combining them:\n",
    "\n",
    "- `top_k_results`:how many results to fetch per query\n",
    "- `doc_content_chars_max`: max characters returned per result (keeps context short & focused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 tools ready: ['wikipedia', 'arxiv', 'pub_med']\n"
     ]
    }
   ],
   "source": [
    "# Tool 1: Wikipedia\n",
    "# Great for general knowledge, concepts, historical facts\n",
    "wikipedia_tool = WikipediaQueryRun(\n",
    "    api_wrapper=WikipediaAPIWrapper(\n",
    "        top_k_results=2,\n",
    "        doc_content_chars_max=1000\n",
    "    )\n",
    ")\n",
    "\n",
    "# Tool 2: ArXiv\n",
    "# Great for AI, machine learning, physics, mathematics research papers\n",
    "arxiv_tool = ArxivQueryRun(\n",
    "    api_wrapper=ArxivAPIWrapper(\n",
    "        top_k_results=2,\n",
    "        doc_content_chars_max=1000\n",
    "    )\n",
    ")\n",
    "\n",
    "# Tool 3: PubMed\n",
    "# Great for medical, biological and health science research\n",
    "pubmed_tool = PubmedQueryRun()\n",
    "\n",
    "# Combine all tools into a list\n",
    "tools = [wikipedia_tool, arxiv_tool, pubmed_tool]\n",
    "\n",
    "print(f\"{len(tools)} tools ready: {[t.name for t in tools]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Agent Setup\n",
    "\n",
    "Here we wire the LLM and tools together into a ReAct agent.\n",
    "\n",
    "Key parameters:\n",
    "- `ZERO_SHOT_REACT_DESCRIPTION`: the agent reads each tool's description to decide which one to use, with no prior examples needed\n",
    "- `verbose=True`: prints the agent's internal reasoning so you can see *why* it picks each tool\n",
    "- `handle_parsing_errors=True`: if the LLM produces a malformed response, the agent retries instead of crashing\n",
    "- `max_iterations=5`: safety limit to prevent infinite loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/santiagomolina/aipm-1711/ds-ai-agent/.venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.3.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "system_prompt = \"\"\"You are a helpful research assistant. \n",
    "You have access to tools to search for information.\n",
    "IMPORTANT RULES:\n",
    "- Use each tool AT MOST ONCE per question\n",
    "- After getting an observation, ALWAYS write your Final Answer immediately\n",
    "- Never repeat the same search twice\n",
    "- Summarize what you found even if the results are imperfect\"\"\"\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    "    max_iterations=4,\n",
    "    early_stopping_method=\"generate\",\n",
    "    agent_kwargs={\"prefix\": system_prompt},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Query 1: General Knowledge (Wikipedia)\n",
    "A factual/historical question, the agent should reach for **Wikipedia**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: The question seems to be about a fundamental concept in physics, so I should use a reliable source for physics-related information.\n",
      "Action: arxiv\n",
      "Action Input: \"theory of relativity\"\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mPublished: 1997-08-21\n",
      "Title: Variational Approach to Quantum Field Theory: Gaussian Approximation and the Perturbative Expansion around It\n",
      "Authors: Jae Hyung Yee\n",
      "Summary: The functional Schrodinger picture formulation of quantum field theory and the variational Gaussian approximation method based on the formulation are briefly reviewed. After presenting recent attempts to improve the variational approximation, we introduce a new systematic method based on the background field method, which enables one to compute the order-by-order correction terms to the Gaussian approximation of the effective action.\n",
      "\n",
      "Published: 2014-03-28\n",
      "Title: The Confrontation between General Relativity and Experiment\n",
      "Authors: Clifford M. Will\n",
      "Summary: The status of experimental tests of general relativity and of theoretical frameworks for analyzing them are reviewed and updated. Einstein's equivalence principle (EEP) is well supported by experiments such as the Eotvos experiment, tests of local Lorentz invariance\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: The observation from the arxiv search does not seem to be relevant to the question about the theory of relativity and its developer. I should try a different approach.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: \"theory of relativity\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Theory of relativity\n",
      "Summary: The theory of relativity comprises two physics theories by Albert Einstein: special relativity and general relativity, proposed and published in 1905 and 1915, respectively. Special relativity applies to all physical phenomena in the absence of gravity. General relativity explains the law of gravitation and its relation to the forces of nature. It applies to the cosmological and astrophysical realm, including astronomy.\n",
      "The theory transformed theoretical physics and astronomy during the 20th century, superseding a 200-year-old theory of mechanics created primarily by Isaac Newton. It introduced concepts including 4-dimensional spacetime as a unified entity of space and time, relativity of simultaneity, kinematic and gravitational time dilation, and length contraction. In the field of physics, relativity improved the science of elementary particles and their fundamental interactions, along with ushering in the nuclear age. With relativity, cosmology a\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: I now know the final answer.\n",
      "\n",
      "Final Answer: The theory of relativity was developed by Albert Einstein, comprising special relativity (1905) and general relativity (1915).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "============================================================\n",
      "FINAL ANSWER:\n",
      "The theory of relativity was developed by Albert Einstein, comprising special relativity (1905) and general relativity (1915).\n"
     ]
    }
   ],
   "source": [
    "result1 = agent.invoke(\"What is the theory of relativity and who developed it?\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL ANSWER:\")\n",
    "print(result1[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Query 2: AI Research (ArXiv)\n",
    "A question about recent academic research, the agent should reach for **ArXiv**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To find recent advances in large language models, I should look for information from a reliable source that covers the latest developments in the field of artificial intelligence and natural language processing.\n",
      "\n",
      "Action: arxiv\n",
      "Action Input: \"recent advances in large language models\"\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mPublished: 2026-02-01\n",
      "Title: Enhancing Human-Like Responses in Large Language Models\n",
      "Authors: Ethem Yağız Çalık, Talha Rüzgar Akkuş\n",
      "Summary: This paper explores the advancements in making large language models (LLMs) more human-like. We focus on techniques that enhance natural language understanding, conversational coherence, and emotional intelligence in AI systems. The study evaluates various approaches, including fine-tuning with diverse datasets, incorporating psychological principles, and designing models that better mimic human reasoning patterns. Our findings demonstrate that these enhancements not only improve user interactions but also open new possibilities for AI applications across different domains. Future work will address the ethical implications and potential biases introduced by these human-like attributes.\n",
      "\n",
      "Published: 2025-09-20\n",
      "Title: Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle\n",
      "Authors: Kelian\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mQuestion: What are recent advances in large language models?\n",
      "Thought: To find recent advances in large language models, I should look for information from a reliable source that covers the latest developments in the field of artificial intelligence and natural language processing.\n",
      "\n",
      "Action: arxiv\n",
      "Action Input: \"recent advances in large language models\"\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mPublished: 2026-02-01\n",
      "Title: Enhancing Human-Like Responses in Large Language Models\n",
      "Authors: Ethem Yağız Çalık, Talha Rüzgar Akkuş\n",
      "Summary: This paper explores the advancements in making large language models (LLMs) more human-like. We focus on techniques that enhance natural language understanding, conversational coherence, and emotional intelligence in AI systems. The study evaluates various approaches, including fine-tuning with diverse datasets, incorporating psychological principles, and designing models that better mimic human reasoning patterns. Our findings demonstrate that these enhancements not only improve user interactions but also open new possibilities for AI applications across different domains. Future work will address the ethical implications and potential biases introduced by these human-like attributes.\n",
      "\n",
      "Published: 2025-09-20\n",
      "Title: Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle\n",
      "Authors: Kelian\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mQuestion: What are recent advances in large language models?\n",
      "Thought: To find recent advances in large language models, I should look for information from a reliable source that covers the latest developments in the field of artificial intelligence and natural language processing.\n",
      "\n",
      "Action: arxiv\n",
      "Action Input: \"recent advances in large language models\"\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mPublished: 2026-02-01\n",
      "Title: Enhancing Human-Like Responses in Large Language Models\n",
      "Authors: Ethem Yağız Çalık, Talha Rüzgar Akkuş\n",
      "Summary: This paper explores the advancements in making large language models (LLMs) more human-like. We focus on techniques that enhance natural language understanding, conversational coherence, and emotional intelligence in AI systems. The study evaluates various approaches, including fine-tuning with diverse datasets, incorporating psychological principles, and designing models that better mimic human reasoning patterns. Our findings demonstrate that these enhancements not only improve user interactions but also open new possibilities for AI applications across different domains. Future work will address the ethical implications and potential biases introduced by these human-like attributes.\n",
      "\n",
      "Published: 2025-09-20\n",
      "Title: Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle\n",
      "Authors: Kelian\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mQuestion: What are recent advances in large language models?\n",
      "Thought: To find recent advances in large language models, I should look for information from a reliable source that covers the latest developments in the field of artificial intelligence and natural language processing.\n",
      "\n",
      "Action: arxiv\n",
      "Action Input: \"recent advances in large language models\"\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mPublished: 2026-02-01\n",
      "Title: Enhancing Human-Like Responses in Large Language Models\n",
      "Authors: Ethem Yağız Çalık, Talha Rüzgar Akkuş\n",
      "Summary: This paper explores the advancements in making large language models (LLMs) more human-like. We focus on techniques that enhance natural language understanding, conversational coherence, and emotional intelligence in AI systems. The study evaluates various approaches, including fine-tuning with diverse datasets, incorporating psychological principles, and designing models that better mimic human reasoning patterns. Our findings demonstrate that these enhancements not only improve user interactions but also open new possibilities for AI applications across different domains. Future work will address the ethical implications and potential biases introduced by these human-like attributes.\n",
      "\n",
      "Published: 2025-09-20\n",
      "Title: Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle\n",
      "Authors: Kelian\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mFinal Answer: \n",
      "Recent advances in large language models include techniques that enhance natural language understanding, conversational coherence, and emotional intelligence in AI systems. These enhancements include fine-tuning with diverse datasets, incorporating psychological principles, and designing models that better mimic human reasoning patterns.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "============================================================\n",
      "FINAL ANSWER:\n",
      "Recent advances in large language models include techniques that enhance natural language understanding, conversational coherence, and emotional intelligence in AI systems. These enhancements include fine-tuning with diverse datasets, incorporating psychological principles, and designing models that better mimic human reasoning patterns.\n"
     ]
    }
   ],
   "source": [
    "result2 = agent.invoke(\"What are recent advances in large language models?\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL ANSWER:\")\n",
    "print(result2[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Query 3: Medical Research (PubMed)\n",
    "A biomedical question, the agent should reach for **PubMed**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL ANSWER:\n",
      "Here is a summary of the research findings in 3-4 clear sentences:\n",
      "\n",
      "A recent study found that brief sleep deprivation after learning alters the synaptic structures of interneurons in the lateral and medial entorhinal cortex, which are key inputs to the hippocampus. Specifically, sleep deprivation reduced the density and size of dendritic spines in these interneurons, but the effects differed between the lateral and medial entorhinal cortex. The study suggests that sleep loss disrupts the balance between excitatory and inhibitory signals in the entorhinal cortex, which may contribute to impaired hippocampus-dependent memory processing. Overall, the findings highlight the importance of sleep for maintaining normal brain function and cognitive processes, particularly after learning.\n"
     ]
    }
   ],
   "source": [
    "  # Bypass the agent entirely — call PubMed directly\n",
    "from langchain_community.tools.pubmed.tool import PubmedQueryRun\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Step 1: Get the raw PubMed results\n",
    "pubmed_tool = PubmedQueryRun()\n",
    "raw_results = pubmed_tool.run(\"sleep deprivation memory\")\n",
    "\n",
    "# Step 2: Ask the LLM to summarize what it found\n",
    "llm = ChatGroq(model=\"llama-3.3-70b-versatile\", temperature=0, max_tokens=256)\n",
    "\n",
    "summary = llm.invoke(\n",
    "    f\"Summarize the following research findings in 3-4 clear sentences:\\n\\n{raw_results}\"\n",
    ")\n",
    "\n",
    "print(\"FINAL ANSWER:\")\n",
    "print(summary.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Trying My Own Query\n",
    "Asking anything, the agent will pick the best tool automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided reference does not contain information about the top 5 best passports in the world in 2026. However, based on my knowledge, the ranking of the best passports can vary depending on the source and criteria used. \n",
      "\n",
      "As of my knowledge cutoff, the top 5 best passports in the world are typically determined by factors such as visa-free travel, travel freedom, and the number of countries that can be visited without a visa. \n",
      "\n",
      "Here are the top 5 best passports in the world in 2026, based on my general knowledge:\n",
      "\n",
      "1. Japanese passport: Offers visa-free travel to over 193 countries.\n",
      "2. Singaporean passport: Allows visa-free travel to over 192 countries.\n",
      "3. South Korean passport: Provides visa-free travel to over 191 countries.\n",
      "4. German passport: Offers visa-free travel to over 190 countries.\n",
      "5. Italian passport: Allows visa-free travel to over 189 countries.\n",
      "\n",
      "Please note that these rankings can change over time and may vary depending on the source and criteria used. It's always best to check with the latest and most reliable sources for the most up-to-date information.\n"
     ]
    }
   ],
   "source": [
    "def research(question: str):\n",
    "    raw = wikipedia_tool.run(question)\n",
    "    summary = llm.invoke(\n",
    "        f\"Answer this question: '{question}'\\n\\nReference:\\n{raw}\\n\\n\"\n",
    "        f\"If the reference doesn't help, use your own knowledge.\"\n",
    "    )\n",
    "    print(summary.content)\n",
    "\n",
    "research(\"What are the top 5 best passports in the world in 2026?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
